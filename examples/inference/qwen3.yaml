model_name_or_path: /app/models/Qwen3-32B
template: qwen3
infer_backend: vllm  # choices: [huggingface, vllm]
vllm_enforce_eager: true
trust_remote_code: true
vllm_config: {
    "pipeline_parallel_size": 1,
    "max_model_len": 32768
}